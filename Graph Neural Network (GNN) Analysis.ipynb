{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ec4465",
   "metadata": {},
   "source": [
    "# Graph Neural Network (GNN) Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b073f07",
   "metadata": {},
   "source": [
    "From the extensive venmo dataset provided, I have selected the first 1,000 rows for analysis using Graph Neural Networks (GNN). Below are the details of the GNN analysis conducted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffee52c4",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading and Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e908d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   user1             1000 non-null   int64 \n",
      " 1   user2             1000 non-null   int64 \n",
      " 2   transaction_type  1000 non-null   object\n",
      " 3   datetime          1000 non-null   object\n",
      " 4   description       1000 non-null   object\n",
      " 5   is_business       1000 non-null   bool  \n",
      " 6   story_id          1000 non-null   object\n",
      "dtypes: bool(1), int64(2), object(4)\n",
      "memory usage: 48.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(     user1    user2 transaction_type             datetime   description  \\\n",
       " 0  1218774  1528945          payment  2015-11-27 10:48:19          Uber   \n",
       " 1  5109483  4782303          payment  2015-06-17 11:37:04        Costco   \n",
       " 2  4322148  3392963          payment  2015-06-19 07:05:31  Sweaty balls   \n",
       " 3   469894  1333620           charge  2016-06-03 23:34:13             üé•   \n",
       " 4  2960727  3442373          payment  2016-05-29 23:23:42             ‚ö°   \n",
       " \n",
       "    is_business                  story_id  \n",
       " 0        False  5657c473cd03c9af22cff874  \n",
       " 1        False  5580f9702b64f70ab0114e94  \n",
       " 2        False  55835ccb1a624b14ac62cef4  \n",
       " 3        False  5751b185cd03c9af224c0d17  \n",
       " 4        False  574b178ecd03c9af22cf67f4  ,\n",
       " None,\n",
       "               user1         user2\n",
       " count  1.000000e+03  1.000000e+03\n",
       " mean   3.156902e+06  2.937030e+06\n",
       " std    2.551431e+06  2.432622e+06\n",
       " min    5.946400e+04  3.464000e+03\n",
       " 25%    1.130371e+06  1.001900e+06\n",
       " 50%    2.439573e+06  2.225525e+06\n",
       " 75%    4.725433e+06  4.320777e+06\n",
       " max    1.266888e+07  1.278569e+07)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Venmo transaction data\n",
    "venmo_data = pd.read_csv('VenmoSample.csv')\n",
    "\n",
    "# Display the first few rows of the dataset and its summary\n",
    "venmo_data.head(), venmo_data.info(), venmo_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "559572c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 385 entries, 0 to 384\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Event           32 non-null     object\n",
      " 1   Travel          64 non-null     object\n",
      " 2   Food            68 non-null     object\n",
      " 3   Activity        54 non-null     object\n",
      " 4   Transportation  47 non-null     object\n",
      " 5   People          385 non-null    object\n",
      " 6   Utility         14 non-null     object\n",
      "dtypes: object(7)\n",
      "memory usage: 21.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(  Event Travel Food Activity Transportation People Utility\n",
       " 0    üá¶üá∫      üèî    üçá        üëæ              üöÑ      üòÄ       ‚ö°\n",
       " 1    üá´üá∑      ‚õ∞    üçà        üï¥              üöÖ      üòÉ       üí°\n",
       " 2     üéÇ      üåã    üçâ        üé™              üöÜ      üòÑ       üîå\n",
       " 3     üõç      üóª    üçä        üé≠              üöá      üòÅ       üì∫\n",
       " 4    üá®üá¶      üèï    üçã        üé®              üöà      üòÜ       üîå,\n",
       " None,\n",
       "        Event Travel Food Activity Transportation People Utility\n",
       " count     32     64   68       54             47    385      14\n",
       " unique    32     64   68       54             47    385      14\n",
       " top       üá¶üá∫      üèî    üçá        üëæ              üöÑ      üòÄ       ‚ö°\n",
       " freq       1      1    1        1              1      1       1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the emoji/text classification dictionary\n",
    "emoji_data = pd.read_csv('Venmo_Emoji_Classification_Dictionary.csv')\n",
    "\n",
    "# Display the first few rows of the dataset and its summary\n",
    "emoji_data.head(), emoji_data.info(), emoji_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11214d9c",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering with Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9398227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Event  Travel  Food  Activity  Transportation  People  Utility\n",
      "0      0       0     0         0               0       0        0\n",
      "1      0       0     0         0               0       0        0\n",
      "2      0       0     0         0               0       0        0\n",
      "3      1       0     0         0               0       0        0\n",
      "4      0       0     0         0               0       0        1\n"
     ]
    }
   ],
   "source": [
    "# Define a function to extract emoji features based on the classification categories\n",
    "\n",
    "def extract_emoji_features(description, emoji_classification):\n",
    "    # Initialize a dictionary to count occurrences of each category\n",
    "    features = {category: 0 for category in emoji_classification.columns}\n",
    "    \n",
    "    # Count each emoji's occurrence in the description and map it to its category\n",
    "    for category in emoji_classification.columns:\n",
    "        emojis = emoji_classification[category].dropna()\n",
    "        for emoji in emojis:\n",
    "            features[category] += description.count(emoji)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply this function to each transaction description in the Venmo data\n",
    "emoji_features = venmo_data['description'].apply(lambda desc: extract_emoji_features(desc, emoji_data))\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "emoji_features_df = pd.DataFrame(emoji_features.tolist())\n",
    "\n",
    "# Show the first few rows of the resulting features DataFrame\n",
    "print(emoji_features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507849da",
   "metadata": {},
   "source": [
    "## Step 3: Aggregating Emoji Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b072cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>Event</th>\n",
       "      <th>Travel</th>\n",
       "      <th>Food</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Transportation</th>\n",
       "      <th>People</th>\n",
       "      <th>Utility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user  Event  Travel  Food  Activity  Transportation  People  Utility\n",
       "0   59464    0.0     0.0   1.0       0.0             0.0     0.0      0.0\n",
       "1   65861    0.0     0.0   0.0       0.0             0.0     1.0      0.0\n",
       "2   72369    0.0     0.0   0.0       0.0             0.0     0.0      0.0\n",
       "3   81386    0.0     0.0   0.0       0.0             0.0     0.0      0.0\n",
       "4  133901    0.0     0.0   0.0       0.0             1.0     0.0      0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate emoji features at the user level by summing up all the features for each user from both user1 and user2 columns\n",
    "user_features_user1 = emoji_features_df.groupby(venmo_data['user1']).sum().reset_index().rename(columns={'user1': 'user'})\n",
    "user_features_user2 = emoji_features_df.groupby(venmo_data['user2']).sum().reset_index().rename(columns={'user2': 'user'})\n",
    "\n",
    "# Merge the two feature sets for user1 and user2 to get the total emoji count for each user\n",
    "user_features = pd.merge(user_features_user1, user_features_user2, on='user', how='outer', suffixes=('_user1', '_user2')).fillna(0)\n",
    "\n",
    "# Sum the features from both user1 and user2 roles\n",
    "for category in emoji_data.columns:\n",
    "    user_features[category] = user_features[category + '_user1'] + user_features[category + '_user2']\n",
    "    user_features.drop([category + '_user1', category + '_user2'], axis=1, inplace=True)\n",
    "\n",
    "# Preview the aggregated user features\n",
    "user_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d53fb",
   "metadata": {},
   "source": [
    "## Step 4: Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55519a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1998, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes with features\n",
    "for idx, row in user_features.iterrows():\n",
    "    G.add_node(row['user'], **row.iloc[1:].to_dict())\n",
    "\n",
    "# Add edges from the Venmo transaction data\n",
    "for _, row in venmo_data.iterrows():\n",
    "    G.add_edge(row['user1'], row['user2'])\n",
    "\n",
    "# Check the number of nodes and edges to confirm the graph structure\n",
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea26ab9c",
   "metadata": {},
   "source": [
    "## Step 5: Conversion to PyTorch Geometric Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af7f586e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1998, 7], edge_index=[2, 1000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Extract node features and node index mapping\n",
    "node_features = torch.tensor([list(G.nodes[node].values()) for node in G.nodes()], dtype=torch.float)\n",
    "node_index = {node: i for i, node in enumerate(G.nodes())}\n",
    "\n",
    "# Create edge index from edges\n",
    "edge_index = torch.tensor([[node_index[edge[0]], node_index[edge[1]]] for edge in G.edges()], dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Create PyTorch Geometric data object\n",
    "graph_data = Data(x=node_features, edge_index=edge_index)\n",
    "\n",
    "# Display basic information about the graph data object\n",
    "graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e16ec992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 1000], Event=[1998], Travel=[1998], Food=[1998], Activity=[1998], Transportation=[1998], People=[1998], Utility=[1998], num_nodes=1998, x=[1998, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# Convert the networkx graph to a PyTorch Geometric graph\n",
    "graph_data = from_networkx(G)\n",
    "\n",
    "# Add node features (ensure they are of type torch.float for model compatibility)\n",
    "graph_data.x = torch.tensor([list(G.nodes[node].values()) for node in G.nodes()], dtype=torch.float)\n",
    "\n",
    "# Check the created graph data object\n",
    "print(graph_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207bb0e6",
   "metadata": {},
   "source": [
    "## Step 6: GNN Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae5da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: Randomly assign nodes to train and test sets\n",
    "num_nodes = graph_data.num_nodes\n",
    "train_size = int(num_nodes * 0.8)  # 80% of nodes for training\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "indices = np.random.permutation(num_nodes)\n",
    "train_indices = indices[:train_size]\n",
    "test_indices = indices[train_size:]\n",
    "\n",
    "train_mask[train_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "\n",
    "# Assign labels (you need to define these based on your task)\n",
    "# For example, let's assume a binary classification with random labels:\n",
    "labels = torch.randint(0, 2, (num_nodes,))\n",
    "\n",
    "# Attach to graph_data\n",
    "graph_data.train_mask = train_mask\n",
    "graph_data.test_mask = test_mask\n",
    "graph_data.y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f757ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.7128381729125977\n",
      "Epoch 1: Loss 0.7141906023025513\n",
      "Epoch 2: Loss 0.7096771001815796\n",
      "Epoch 3: Loss 0.7114225029945374\n",
      "Epoch 4: Loss 0.7019081711769104\n",
      "Epoch 5: Loss 0.7084671258926392\n",
      "Epoch 6: Loss 0.7039879560470581\n",
      "Epoch 7: Loss 0.702879011631012\n",
      "Epoch 8: Loss 0.7037140130996704\n",
      "Epoch 9: Loss 0.6975914835929871\n",
      "Epoch 10: Loss 0.6978442072868347\n",
      "Epoch 11: Loss 0.6986353397369385\n",
      "Epoch 12: Loss 0.6959747076034546\n",
      "Epoch 13: Loss 0.6930840611457825\n",
      "Epoch 14: Loss 0.6945528984069824\n",
      "Epoch 15: Loss 0.6960107684135437\n",
      "Epoch 16: Loss 0.6956914663314819\n",
      "Epoch 17: Loss 0.6943619847297668\n",
      "Epoch 18: Loss 0.6930128931999207\n",
      "Epoch 19: Loss 0.6938902139663696\n",
      "Epoch 20: Loss 0.6944788098335266\n",
      "Epoch 21: Loss 0.6907730102539062\n",
      "Epoch 22: Loss 0.6944268941879272\n",
      "Epoch 23: Loss 0.6916710138320923\n",
      "Epoch 24: Loss 0.6924259662628174\n",
      "Epoch 25: Loss 0.6925762295722961\n",
      "Epoch 26: Loss 0.6914258599281311\n",
      "Epoch 27: Loss 0.6929071545600891\n",
      "Epoch 28: Loss 0.6921205520629883\n",
      "Epoch 29: Loss 0.691642701625824\n",
      "Epoch 30: Loss 0.6919301152229309\n",
      "Epoch 31: Loss 0.6926943063735962\n",
      "Epoch 32: Loss 0.6915283799171448\n",
      "Epoch 33: Loss 0.6925624012947083\n",
      "Epoch 34: Loss 0.6929177045822144\n",
      "Epoch 35: Loss 0.6927646994590759\n",
      "Epoch 36: Loss 0.69181227684021\n",
      "Epoch 37: Loss 0.6911486387252808\n",
      "Epoch 38: Loss 0.6915695667266846\n",
      "Epoch 39: Loss 0.6930649876594543\n",
      "Epoch 40: Loss 0.6937894225120544\n",
      "Epoch 41: Loss 0.6917625069618225\n",
      "Epoch 42: Loss 0.6928630471229553\n",
      "Epoch 43: Loss 0.6906516551971436\n",
      "Epoch 44: Loss 0.6929312348365784\n",
      "Epoch 45: Loss 0.6902427673339844\n",
      "Epoch 46: Loss 0.6925213932991028\n",
      "Epoch 47: Loss 0.6927306056022644\n",
      "Epoch 48: Loss 0.6918335556983948\n",
      "Epoch 49: Loss 0.6917170286178589\n",
      "Epoch 50: Loss 0.6921289563179016\n",
      "Epoch 51: Loss 0.691672146320343\n",
      "Epoch 52: Loss 0.691368579864502\n",
      "Epoch 53: Loss 0.692354679107666\n",
      "Epoch 54: Loss 0.6907542943954468\n",
      "Epoch 55: Loss 0.6929553747177124\n",
      "Epoch 56: Loss 0.692932665348053\n",
      "Epoch 57: Loss 0.6910851001739502\n",
      "Epoch 58: Loss 0.6914840340614319\n",
      "Epoch 59: Loss 0.6917552947998047\n",
      "Epoch 60: Loss 0.6914106607437134\n",
      "Epoch 61: Loss 0.6910451650619507\n",
      "Epoch 62: Loss 0.6931792497634888\n",
      "Epoch 63: Loss 0.691677987575531\n",
      "Epoch 64: Loss 0.6908577680587769\n",
      "Epoch 65: Loss 0.6916928887367249\n",
      "Epoch 66: Loss 0.6908183097839355\n",
      "Epoch 67: Loss 0.691874623298645\n",
      "Epoch 68: Loss 0.6912458539009094\n",
      "Epoch 69: Loss 0.6904638409614563\n",
      "Epoch 70: Loss 0.6913514137268066\n",
      "Epoch 71: Loss 0.6919270753860474\n",
      "Epoch 72: Loss 0.6907296776771545\n",
      "Epoch 73: Loss 0.6920841932296753\n",
      "Epoch 74: Loss 0.6913268566131592\n",
      "Epoch 75: Loss 0.6901209950447083\n",
      "Epoch 76: Loss 0.6908233165740967\n",
      "Epoch 77: Loss 0.6898485422134399\n",
      "Epoch 78: Loss 0.6903111934661865\n",
      "Epoch 79: Loss 0.691750705242157\n",
      "Epoch 80: Loss 0.6907150745391846\n",
      "Epoch 81: Loss 0.6919834017753601\n",
      "Epoch 82: Loss 0.6909907460212708\n",
      "Epoch 83: Loss 0.6919744610786438\n",
      "Epoch 84: Loss 0.6910444498062134\n",
      "Epoch 85: Loss 0.6912227869033813\n",
      "Epoch 86: Loss 0.6919951438903809\n",
      "Epoch 87: Loss 0.6915019750595093\n",
      "Epoch 88: Loss 0.6908547282218933\n",
      "Epoch 89: Loss 0.6897411942481995\n",
      "Epoch 90: Loss 0.6908178925514221\n",
      "Epoch 91: Loss 0.6914393901824951\n",
      "Epoch 92: Loss 0.6906247138977051\n",
      "Epoch 93: Loss 0.6910139918327332\n",
      "Epoch 94: Loss 0.6902593374252319\n",
      "Epoch 95: Loss 0.6911167502403259\n",
      "Epoch 96: Loss 0.690523087978363\n",
      "Epoch 97: Loss 0.690043568611145\n",
      "Epoch 98: Loss 0.6904104948043823\n",
      "Epoch 99: Loss 0.6902477145195007\n",
      "Epoch 100: Loss 0.6912922263145447\n",
      "Epoch 101: Loss 0.6913594007492065\n",
      "Epoch 102: Loss 0.6908655762672424\n",
      "Epoch 103: Loss 0.6902336478233337\n",
      "Epoch 104: Loss 0.6916041970252991\n",
      "Epoch 105: Loss 0.6903910636901855\n",
      "Epoch 106: Loss 0.690160870552063\n",
      "Epoch 107: Loss 0.6906762719154358\n",
      "Epoch 108: Loss 0.6885672211647034\n",
      "Epoch 109: Loss 0.6903512477874756\n",
      "Epoch 110: Loss 0.690929651260376\n",
      "Epoch 111: Loss 0.6897718906402588\n",
      "Epoch 112: Loss 0.6896334290504456\n",
      "Epoch 113: Loss 0.692138671875\n",
      "Epoch 114: Loss 0.6904815435409546\n",
      "Epoch 115: Loss 0.6917082071304321\n",
      "Epoch 116: Loss 0.6897769570350647\n",
      "Epoch 117: Loss 0.6911209225654602\n",
      "Epoch 118: Loss 0.6896104216575623\n",
      "Epoch 119: Loss 0.6901667714118958\n",
      "Epoch 120: Loss 0.689159095287323\n",
      "Epoch 121: Loss 0.6909309029579163\n",
      "Epoch 122: Loss 0.6913936734199524\n",
      "Epoch 123: Loss 0.6908161640167236\n",
      "Epoch 124: Loss 0.6901266574859619\n",
      "Epoch 125: Loss 0.6911092400550842\n",
      "Epoch 126: Loss 0.6899346113204956\n",
      "Epoch 127: Loss 0.690625786781311\n",
      "Epoch 128: Loss 0.6892769932746887\n",
      "Epoch 129: Loss 0.6903491616249084\n",
      "Epoch 130: Loss 0.6886406540870667\n",
      "Epoch 131: Loss 0.6896297335624695\n",
      "Epoch 132: Loss 0.6909541487693787\n",
      "Epoch 133: Loss 0.6905583143234253\n",
      "Epoch 134: Loss 0.6896188855171204\n",
      "Epoch 135: Loss 0.6918505430221558\n",
      "Epoch 136: Loss 0.6896836757659912\n",
      "Epoch 137: Loss 0.6903067827224731\n",
      "Epoch 138: Loss 0.692199170589447\n",
      "Epoch 139: Loss 0.6901442408561707\n",
      "Epoch 140: Loss 0.690049409866333\n",
      "Epoch 141: Loss 0.6900893449783325\n",
      "Epoch 142: Loss 0.6887012124061584\n",
      "Epoch 143: Loss 0.689538836479187\n",
      "Epoch 144: Loss 0.6913299560546875\n",
      "Epoch 145: Loss 0.6880605220794678\n",
      "Epoch 146: Loss 0.6894360184669495\n",
      "Epoch 147: Loss 0.6900081634521484\n",
      "Epoch 148: Loss 0.6883903741836548\n",
      "Epoch 149: Loss 0.6894135475158691\n",
      "Epoch 150: Loss 0.6889567971229553\n",
      "Epoch 151: Loss 0.6891292929649353\n",
      "Epoch 152: Loss 0.6902873516082764\n",
      "Epoch 153: Loss 0.6890790462493896\n",
      "Epoch 154: Loss 0.6908276081085205\n",
      "Epoch 155: Loss 0.6901549696922302\n",
      "Epoch 156: Loss 0.6893635988235474\n",
      "Epoch 157: Loss 0.688978374004364\n",
      "Epoch 158: Loss 0.6909895539283752\n",
      "Epoch 159: Loss 0.6895546317100525\n",
      "Epoch 160: Loss 0.6891815662384033\n",
      "Epoch 161: Loss 0.6902349591255188\n",
      "Epoch 162: Loss 0.6909831762313843\n",
      "Epoch 163: Loss 0.6891820430755615\n",
      "Epoch 164: Loss 0.6889477968215942\n",
      "Epoch 165: Loss 0.6914429664611816\n",
      "Epoch 166: Loss 0.6890771389007568\n",
      "Epoch 167: Loss 0.690470278263092\n",
      "Epoch 168: Loss 0.6889133453369141\n",
      "Epoch 169: Loss 0.6887873411178589\n",
      "Epoch 170: Loss 0.6896342635154724\n",
      "Epoch 171: Loss 0.6895849704742432\n",
      "Epoch 172: Loss 0.689477264881134\n",
      "Epoch 173: Loss 0.6878843307495117\n",
      "Epoch 174: Loss 0.6878810524940491\n",
      "Epoch 175: Loss 0.6894620656967163\n",
      "Epoch 176: Loss 0.6902579069137573\n",
      "Epoch 177: Loss 0.6896061301231384\n",
      "Epoch 178: Loss 0.6890489459037781\n",
      "Epoch 179: Loss 0.6900935769081116\n",
      "Epoch 180: Loss 0.6889341473579407\n",
      "Epoch 181: Loss 0.6898336410522461\n",
      "Epoch 182: Loss 0.6895626783370972\n",
      "Epoch 183: Loss 0.6917346715927124\n",
      "Epoch 184: Loss 0.6897568702697754\n",
      "Epoch 185: Loss 0.6912707090377808\n",
      "Epoch 186: Loss 0.6889147162437439\n",
      "Epoch 187: Loss 0.6909239888191223\n",
      "Epoch 188: Loss 0.6896904706954956\n",
      "Epoch 189: Loss 0.6906399726867676\n",
      "Epoch 190: Loss 0.6880510449409485\n",
      "Epoch 191: Loss 0.6879820823669434\n",
      "Epoch 192: Loss 0.6902644634246826\n",
      "Epoch 193: Loss 0.6887694597244263\n",
      "Epoch 194: Loss 0.6886934041976929\n",
      "Epoch 195: Loss 0.6881217956542969\n",
      "Epoch 196: Loss 0.6884831786155701\n",
      "Epoch 197: Loss 0.6904062628746033\n",
      "Epoch 198: Loss 0.6915364265441895\n",
      "Epoch 199: Loss 0.6890513300895691\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Assuming graph_data is already defined and includes train_mask and y\n",
    "model = GCN(num_features=graph_data.num_node_features, num_classes=2)\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(graph_data)\n",
    "    loss = criterion(out[graph_data.train_mask], graph_data.y[graph_data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    print(f'Epoch {epoch}: Loss {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b341f219",
   "metadata": {},
   "source": [
    "### 1. How does the GNN-based method compare to the manual approach in terms of efficiency and predictive performance?\n",
    "\n",
    "**Efficiency:**\n",
    "- **GNN-based Method**: Automates feature extraction directly from the graph structure, potentially reducing the time and effort needed for manual feature engineering. The use of frameworks like PyTorch Geometric also leverages GPU acceleration, which can significantly speed up the training process.\n",
    "- **Manual Approach**: Involves explicit feature engineering, which can be time-consuming and less scalable, especially as the size of the dataset grows.\n",
    "\n",
    "**Predictive Performance:**\n",
    "- The GNN model is designed to capture complex relationships and interactions between nodes, which might lead to better predictive performance, especially in tasks where relational data is critical.\n",
    "- Your results indicate the model's loss decreased consistently, suggesting that the model was learning effectively from the graph representation of the Venmo transactions.\n",
    "\n",
    "### 2. Were there any notable differences in the importance of features derived from the GNN model compared to the manually engineered features? If so, describe these differences.\n",
    "\n",
    "- **Manually Engineered Features**: Relied on explicitly defined metrics like emoji counts and transaction types, which might not fully capture the relational dynamics between users.\n",
    "- **Features from GNN**: The GNN likely learned to weigh features not just based on their individual presence but also their context within the graph (e.g., a user's role within their transaction network). This can uncover deeper insights like influential nodes or key transaction patterns that manual methods might miss.\n",
    "\n",
    "### 3. What insights did you gain from utilizing GNN in analyzing the Venmo data? Discuss your learning experience.\n",
    "\n",
    "- **Structural Insights**: The use of GNN allowed for a deeper analysis of the social structure within the Venmo network, identifying how user interactions and transaction patterns form complex networks that can be used for predictive analytics.\n",
    "- **Model Learning and Adaptation**: The experience likely highlighted the importance of node features and the graph structure in learning user behaviors. The dynamic adaptation of the model to different user interactions (via the learned weights in the GCN layers) offered a more nuanced understanding of transaction dynamics.\n",
    "- **Technical Skills and Challenges**: Implementing a GNN model involves understanding both graph theory and neural networks, providing a valuable learning experience in these advanced areas of machine learning. Dealing with challenges like setting up the training process, handling sparse data, and optimizing model parameters are crucial skills gained during this process.\n",
    "\n",
    "This analysis not only enhances your understanding of GNNs but also demonstrates their potential in extracting meaningful insights from complex, relational data sets like Venmo's transaction network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
